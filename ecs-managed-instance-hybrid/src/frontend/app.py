"""
vLLM Spot Inference Frontend Application

stliteã§å‹•ä½œã™ã‚‹Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€‚
ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªå‹•é€ä¿¡ã—ã€vLLM APIã‹ã‚‰ã®å¿œç­”ã‚’è¡¨ç¤ºã™ã‚‹ã€‚

Note: stlite (Pyodide) ã§ã¯ time.sleep() ãŒå‹•ä½œã—ãªã„ãŸã‚ã€
asyncio.sleep() ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
"""

import streamlit as st
import asyncio
import random
from datetime import datetime

# ============================================
# Configuration
# ============================================

# äº‹å‰å®šç¾©ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé…åˆ— (Requirement 4.1)
PROMPTS = [
    "æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å­¦ã¶ã‚³ãƒ„ã‚’æ•™ãˆã¦ãã ã•ã„",
    "å¥åº·çš„ãªæœé£Ÿã®ãƒ¬ã‚·ãƒ”ã‚’ææ¡ˆã—ã¦ãã ã•ã„",
    "AIã®æœªæ¥ã«ã¤ã„ã¦ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„",
    "åŠ¹ç‡çš„ãªæ™‚é–“ç®¡ç†ã®æ–¹æ³•ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„",
    "ç’°å¢ƒå•é¡Œã«ã¤ã„ã¦ä¸€è¨€ã§èª¬æ˜ã—ã¦ãã ã•ã„",
    "ãŠã™ã™ã‚ã®æœ¬ã‚’1å†Šç´¹ä»‹ã—ã¦ãã ã•ã„",
    "ã‚¹ãƒˆãƒ¬ã‚¹è§£æ¶ˆæ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
    "æ–°ã—ã„è¶£å‘³ã‚’å§‹ã‚ã‚‹ãªã‚‰ãŠã™ã™ã‚ã¯ï¼Ÿ",
    "ä»Šæ—¥ã®å¤©æ°—ã«åˆã†æœè£…ã‚’ææ¡ˆã—ã¦ãã ã•ã„",
]

# APIè¨­å®š
API_ENDPOINT = st.secrets.get("api_endpoint", "http://localhost:8000")
DELAY_SECONDS = 2
MAX_TOKENS = 512

# ============================================
# Session State Initialization (Requirement 3.2)
# ============================================

if "history" not in st.session_state:
    st.session_state.history = []

if "is_running" not in st.session_state:
    st.session_state.is_running = False

if "last_prompt" not in st.session_state:
    st.session_state.last_prompt = None

if "current_prompt" not in st.session_state:
    st.session_state.current_prompt = None

if "current_response" not in st.session_state:
    st.session_state.current_response = None


# ============================================
# Prompt Selection Logic (Requirements 4.2, 4.3)
# ============================================

def select_random_prompt(prompts: list[str], last_prompt: str | None) -> str:
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé…åˆ—ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã™ã‚‹ã€‚
    é€£ç¶šé‡è¤‡ã‚’å›é¿ã™ã‚‹ï¼ˆé…åˆ—ã«è¤‡æ•°ã®è¦ç´ ãŒã‚ã‚‹å ´åˆï¼‰ã€‚
    
    Args:
        prompts: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é…åˆ—
        last_prompt: å‰å›é¸æŠã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    
    Returns:
        é¸æŠã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    if len(prompts) == 0:
        raise ValueError("Prompts array cannot be empty")
    
    if len(prompts) == 1:
        return prompts[0]
    
    # é€£ç¶šé‡è¤‡ã‚’å›é¿
    available_prompts = [p for p in prompts if p != last_prompt]
    return random.choice(available_prompts)


# ============================================
# API Call Logic (Requirement 1.2, 1.3)
# ============================================

def call_inference_api(prompt: str, endpoint: str = API_ENDPOINT) -> dict:
    """
    vLLM APIã‚’å‘¼ã³å‡ºã—ã¦æ¨è«–çµæœã‚’å–å¾—ã™ã‚‹ã€‚
    
    Args:
        prompt: é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        endpoint: APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    Returns:
        APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆresponse, latency_msï¼‰
    """
    import pyodide.http
    import json
    
    start_time = asyncio.get_event_loop().time()
    
    try:
        response = pyodide.http.open_url(
            f"{endpoint}/v1/chat/completions",
            method="POST",
            body=json.dumps({
                "model": "Qwen/Qwen3-4B",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": MAX_TOKENS,
            }),
            headers={"Content-Type": "application/json"},
        )
        
        latency_ms = (asyncio.get_event_loop().time() - start_time) * 1000
        data = json.loads(response.read())
        
        # OpenAIäº’æ›ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        response_text = data["choices"][0]["message"]["content"]
        
        return {
            "response": response_text,
            "latency_ms": latency_ms,
            "success": True,
            "error": None,
        }
    except Exception as e:
        latency_ms = (asyncio.get_event_loop().time() - start_time) * 1000
        return {
            "response": None,
            "latency_ms": latency_ms,
            "success": False,
            "error": str(e),
        }


# ============================================
# History Management (Requirement 3.2, 3.5)
# ============================================

def add_to_history(prompt: str, response: str, latency_ms: float) -> None:
    """
    å±¥æ­´ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”ã®ãƒšã‚¢ã‚’è¿½åŠ ã™ã‚‹ã€‚
    
    Args:
        prompt: é€ä¿¡ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        response: å—ä¿¡ã—ãŸå¿œç­”
        latency_ms: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆãƒŸãƒªç§’ï¼‰
    """
    history_item = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "prompt": prompt,
        "response": response,
        "latency_ms": latency_ms,
    }
    st.session_state.history.append(history_item)


# ============================================
# Control Functions (Requirement 3.6)
# ============================================

def start_inference():
    """è‡ªå‹•æ¨è«–ã‚’é–‹å§‹ã™ã‚‹"""
    st.session_state.is_running = True


def stop_inference():
    """è‡ªå‹•æ¨è«–ã‚’åœæ­¢ã™ã‚‹"""
    st.session_state.is_running = False


# ============================================
# Main Inference Loop (Requirements 3.3, 3.4, 3.5)
# ============================================

def run_inference_cycle():
    """
    1å›ã®æ¨è«–ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    - ãƒ©ãƒ³ãƒ€ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸æŠ
    - APIã‚’å‘¼ã³å‡ºã—
    - å±¥æ­´ã«è¿½åŠ 
    - 2ç§’å¾…æ©Ÿ
    """
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠ
    prompt = select_random_prompt(PROMPTS, st.session_state.last_prompt)
    st.session_state.current_prompt = prompt
    st.session_state.last_prompt = prompt
    
    # APIå‘¼ã³å‡ºã—
    result = call_inference_api(prompt)
    
    if result["success"]:
        st.session_state.current_response = result["response"]
        add_to_history(prompt, result["response"], result["latency_ms"])
    else:
        st.session_state.current_response = f"Error: {result['error']}"
        add_to_history(prompt, f"Error: {result['error']}", result["latency_ms"])
    
    # 2ç§’å¾…æ©Ÿ (Requirement 3.4)
    time.sleep(DELAY_SECONDS)


# ============================================
# UI Components
# ============================================

def render_ui():
    """ãƒ¡ã‚¤ãƒ³UIã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã™ã‚‹"""
    st.title("ğŸ¤– vLLM Spot Inference Demo")
    st.markdown("Qwen/Qwen3-4Bãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸè‡ªå‹•æ¨è«–ãƒ‡ãƒ¢")
    
    # ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒœã‚¿ãƒ³ (Requirement 3.6)
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        if st.button("â–¶ï¸ é–‹å§‹", disabled=st.session_state.is_running):
            start_inference()
            st.rerun()
    
    with col2:
        if st.button("â¹ï¸ åœæ­¢", disabled=not st.session_state.is_running):
            stop_inference()
            st.rerun()
    
    with col3:
        status = "ğŸŸ¢ å®Ÿè¡Œä¸­" if st.session_state.is_running else "ğŸ”´ åœæ­¢ä¸­"
        st.markdown(f"**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** {status}")
    
    st.divider()
    
    # ç¾åœ¨ã®å‡¦ç†çŠ¶æ³ (Requirement 3.5)
    st.subheader("ğŸ“ ç¾åœ¨ã®å‡¦ç†")
    
    if st.session_state.current_prompt:
        st.markdown(f"**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:** {st.session_state.current_prompt}")
        
        if st.session_state.current_response:
            st.markdown("**å¿œç­”:**")
            st.info(st.session_state.current_response)
    else:
        st.markdown("_å¾…æ©Ÿä¸­..._")
    
    st.divider()
    
    # å±¥æ­´è¡¨ç¤º (Requirement 3.2)
    st.subheader("ğŸ“œ å±¥æ­´")
    
    if st.session_state.history:
        # æ–°ã—ã„é †ã«è¡¨ç¤º
        for item in reversed(st.session_state.history[-10:]):
            with st.expander(f"ğŸ• {item['timestamp']} - {item['prompt'][:30]}..."):
                st.markdown(f"**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:** {item['prompt']}")
                st.markdown(f"**å¿œç­”:** {item['response']}")
                st.markdown(f"**ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·:** {item['latency_ms']:.1f}ms")
    else:
        st.markdown("_å±¥æ­´ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“_")


# ============================================
# Main Application
# ============================================

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    st.set_page_config(
        page_title="vLLM Spot Inference",
        page_icon="ğŸ¤–",
        layout="wide",
    )
    
    render_ui()
    
    # è‡ªå‹•æ¨è«–ãƒ«ãƒ¼ãƒ— (Requirement 3.3)
    if st.session_state.is_running:
        run_inference_cycle()
        st.rerun()


if __name__ == "__main__":
    main()
