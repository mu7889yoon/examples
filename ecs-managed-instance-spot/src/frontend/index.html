<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <title>vLLM Spot Inference Demo</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@stlite/browser@0.90.0/build/stlite.css" />
</head>

<body>
    <div id="root"></div>
    <script type="module">
        import { mount } from "https://cdn.jsdelivr.net/npm/@stlite/browser@0.90.0/build/stlite.js"

        // API Endpoint - CloudFrontçµŒç”±ã§åŒä¸€ã‚ªãƒªã‚¸ãƒ³ï¼ˆ/v1ãƒ‘ã‚¹ã¯ãã®ã¾ã¾ALBã«è»¢é€ï¼‰
        const origin = window.location.origin;
        const API_ENDPOINT = origin;

        mount(
            {
                requirements: ["requests", "asyncio"],
                entrypoint: "app.py",
                files: {
                    "app.py": `
import streamlit as st
import json
import requests
import time
import asyncio
import random

# ============================================
# Configuration
# ============================================

API_ENDPOINT = "${API_ENDPOINT}"
MODEL_NAME = "Qwen/Qwen3-4B"
MAX_TOKENS = 512
DELAY_SECONDS = 2

# äº‹å‰å®šç¾©ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé…åˆ— (Requirement 4.1)
PROMPTS = [
    "æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å­¦ã¶ã‚³ãƒ„ã‚’æ•™ãˆã¦ãã ã•ã„",
    "å¥åº·çš„ãªæœé£Ÿã®ãƒ¬ã‚·ãƒ”ã‚’ææ¡ˆã—ã¦ãã ã•ã„",
    "AIã®æœªæ¥ã«ã¤ã„ã¦ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„",
    "åŠ¹ç‡çš„ãªæ™‚é–“ç®¡ç†ã®æ–¹æ³•ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„",
    "ç’°å¢ƒå•é¡Œã«ã¤ã„ã¦ä¸€è¨€ã§èª¬æ˜ã—ã¦ãã ã•ã„",
    "ãŠã™ã™ã‚ã®æœ¬ã‚’1å†Šç´¹ä»‹ã—ã¦ãã ã•ã„",
    "ã‚¹ãƒˆãƒ¬ã‚¹è§£æ¶ˆæ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
    "æ–°ã—ã„è¶£å‘³ã‚’å§‹ã‚ã‚‹ãªã‚‰ãŠã™ã™ã‚ã¯ï¼Ÿ",
    "ä»Šæ—¥ã®å¤©æ°—ã«åˆã†æœè£…ã‚’ææ¡ˆã—ã¦ãã ã•ã„",
]

# ============================================
# Session State Initialization
# ============================================

if "messages" not in st.session_state:
    st.session_state.messages = []

if "history" not in st.session_state:
    st.session_state.history = []

if "is_running" not in st.session_state:
    st.session_state.is_running = False

if "last_prompt" not in st.session_state:
    st.session_state.last_prompt = None

if "pending_inference" not in st.session_state:
    st.session_state.pending_inference = False

# ============================================
# Helper Functions
# ============================================

def select_random_prompt(prompts, last_prompt):
    """é€£ç¶šé‡è¤‡ã‚’å›é¿ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸æŠ"""
    if len(prompts) == 0:
        return None
    if len(prompts) == 1:
        return prompts[0]
    available = [p for p in prompts if p != last_prompt]
    return random.choice(available)


def call_vllm_api(prompt):
    """vLLM OpenAIäº’æ›APIã‚’å‘¼ã³å‡ºã—ï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰"""
    try:
        start_time = time.time()
        response = requests.post(
            f"{API_ENDPOINT}/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": MODEL_NAME,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": MAX_TOKENS,
            },
        )
        response.raise_for_status()
        latency_ms = (time.time() - start_time) * 1000
        data = response.json()
        content = data["choices"][0]["message"]["content"]
        return {"content": content, "latency_ms": latency_ms, "success": True}
    except Exception as e:
        return {"content": f"ã‚¨ãƒ©ãƒ¼: {str(e)}", "latency_ms": 0, "success": False}


# ============================================
# UI
# ============================================

st.set_page_config(
    page_title="vLLM Spot Inference",
    page_icon="ğŸ¤–",
    layout="wide",
)

st.title("ğŸ¤– vLLM Spot Inference Demo")
st.markdown(f"**Model:** {MODEL_NAME} | **Endpoint:** {API_ENDPOINT}")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("âš™ï¸ ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("â–¶ï¸ é–‹å§‹", disabled=st.session_state.is_running, use_container_width=True):
            st.session_state.is_running = True
            st.session_state.pending_inference = True
            st.rerun()
    
    with col2:
        if st.button("â¹ï¸ åœæ­¢", disabled=not st.session_state.is_running, use_container_width=True):
            st.session_state.is_running = False
            st.session_state.pending_inference = False
            st.rerun()
    
    status = "ğŸŸ¢ å®Ÿè¡Œä¸­" if st.session_state.is_running else "ğŸ”´ åœæ­¢ä¸­"
    st.markdown(f"**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** {status}")
    
    st.divider()
    
    if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚’ã‚¯ãƒªã‚¢", use_container_width=True):
        st.session_state.history = []
        st.session_state.messages = []
        st.rerun()
    
    st.divider()
    st.markdown("### ğŸ“Š çµ±è¨ˆ")
    st.markdown(f"- æ¨è«–å›æ•°: {len(st.session_state.history)}")
    if st.session_state.history:
        avg_latency = sum(h.get('latency_ms', 0) for h in st.session_state.history) / len(st.session_state.history)
        st.markdown(f"- å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {avg_latency:.0f}ms")

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
tab1, tab2 = st.tabs(["ğŸ’¬ ãƒãƒ£ãƒƒãƒˆ", "ğŸ“œ å±¥æ­´"])

with tab1:
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # è‡ªå‹•æ¨è«–å‡¦ç†
    if st.session_state.pending_inference and st.session_state.is_running:
        # ãƒ©ãƒ³ãƒ€ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠ
        prompt = select_random_prompt(PROMPTS, st.session_state.last_prompt)
        st.session_state.last_prompt = prompt
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”
        with st.chat_message("assistant"):
            with st.spinner("æ¨è«–ä¸­..."):
                result = call_vllm_api(prompt)
                st.markdown(result["content"])
        
        # å±¥æ­´ã«è¿½åŠ 
        st.session_state.messages.append({"role": "assistant", "content": result["content"]})
        st.session_state.history.append({
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "prompt": prompt,
            "response": result["content"],
            "latency_ms": result["latency_ms"],
        })
        
        # 2ç§’å¾…æ©Ÿå¾Œã«æ¬¡ã®æ¨è«–
        if st.session_state.is_running:
            await asyncio.sleep(DELAY_SECONDS)
            st.rerun()

with tab2:
    if st.session_state.history:
        for i, item in enumerate(reversed(st.session_state.history[-20:])):
            with st.expander(f"ğŸ• {item['timestamp']} - {item['prompt'][:40]}..."):
                st.markdown(f"**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:** {item['prompt']}")
                st.markdown(f"**å¿œç­”:** {item['response']}")
                st.markdown(f"**ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·:** {item['latency_ms']:.0f}ms")
    else:
        st.info("å±¥æ­´ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚ã€Œé–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦è‡ªå‹•æ¨è«–ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")

# æ‰‹å‹•å…¥åŠ›
if prompt := st.chat_input("æ‰‹å‹•ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ï¼ˆè‡ªå‹•æ¨è«–ã¨ã¯åˆ¥ï¼‰"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("æ¨è«–ä¸­..."):
            result = call_vllm_api(prompt)
            st.markdown(result["content"])
    
    st.session_state.messages.append({"role": "assistant", "content": result["content"]})
    st.rerun()
`,
                },
            },
            document.getElementById("root")
        )
    </script>
</body>

</html>
